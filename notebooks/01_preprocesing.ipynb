{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "088ba658",
   "metadata": {},
   "source": [
    "# 01 Preprocessing\n",
    "\n",
    "Goal: Prepare data to be used in model training.\n",
    "1. Combining datasets into one homogenous human-readable dataset, ensuring no empty cells, with the text **Article** and boolean **Truth** columns.\n",
    "2. Remove any punctuation and error-prone text.\n",
    "3. Tokenize the data for NLP.\n",
    "\n",
    "Output:\n",
    "1. Human-readable dataset\n",
    "2. Reduced dataset\n",
    "3. Tokenized dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc754917",
   "metadata": {},
   "source": [
    "## 01.1 Human-Readable Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322993b1",
   "metadata": {},
   "source": [
    "First, we'll reduce the raw FakeNewsNet dataset to only the columns we need, with a boolean truth label added manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5b2373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c609aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_files = [\n",
    "    '../data/raw/FakeNewsNet/gossipcop_fake.csv',\n",
    "    '../data/raw/FakeNewsNet/gossipcop_real.csv',\n",
    "    '../data/raw/FakeNewsNet/politifact_fake.csv',\n",
    "    '../data/raw/FakeNewsNet/politifact_real.csv'\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for file in fnn_files:\n",
    "    temp_df = pd.read_csv(file)\n",
    "\n",
    "    # Determine truth value: 1 for real, 0 for fake\n",
    "    truth = 1 if 'real' in file else 0\n",
    "    \n",
    "    # Some files may use different column names for the article text\n",
    "    if 'title' in temp_df.columns:\n",
    "        titles = temp_df['title']\n",
    "    elif 'text' in temp_df.columns:\n",
    "        titles = temp_df['text']\n",
    "    else:\n",
    "        continue  # skip if no suitable column\n",
    "    df_subset = pd.DataFrame({'Article': titles, 'Truth': truth})\n",
    "    dfs.append(df_subset)\n",
    "\n",
    "joint_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d5ecd5",
   "metadata": {},
   "source": [
    "Next, we'll do the same for the ISOT dataset, concatenating the title and body of the article together since it's present in the dataset already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99719a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "isot_files = [\n",
    "    '../data/raw/ISOTFakeNewsDataset/True.csv',\n",
    "    '../data/raw/ISOTFakeNewsDataset/Fake.csv'\n",
    "]\n",
    "\n",
    "for file in isot_files:\n",
    "    temp_df = pd.read_csv(file)\n",
    "    truth = 1 if 'True' in file else 0 # a little unecessary but whatever\n",
    "\n",
    "    # Concatenate title and text for ISOT dataset\n",
    "    articles = temp_df['title'] + ' ' + temp_df['text']\n",
    "    df_subset = pd.DataFrame({'Article': articles, 'Truth': truth})\n",
    "    dfs.append(df_subset)\n",
    "joint_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd811124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02bab2c",
   "metadata": {},
   "source": [
    "## 01.2 Reduced Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ff2288",
   "metadata": {},
   "source": [
    "Next, we'll check some traits of this data for potential points of failure. First, check for blank article entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b03f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_df.shape\n",
    "joint_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8c48a8",
   "metadata": {},
   "source": [
    "Next, we want to remove punctuation and unusual characters. We'll use a standard procedure for NLP cleaning:\n",
    "1. Lowercasing\n",
    "2. Removal of punctuation and special characters\n",
    "3. Removal of extra whitespace\n",
    "4. Removal of stopwords (such as 'the', 'is', or 'and')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ff574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercasing\n",
    "joint_df['Article'] = joint_df['Article'].str.lower()\n",
    "\n",
    "# Removal of punctuation and special characters\n",
    "import re\n",
    "joint_df['Article'] = joint_df['Article'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
    "\n",
    "# Removal of extra whitespace\n",
    "joint_df['Article'] = joint_df['Article'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "\n",
    "# Removal of stopwords (such as 'the', 'is', or 'and')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
